book1, 시작하세요! 엘라스틱서치:루씬 기반의 실시간 오픈소스 검색엔진
===

NOTE: 얉게 훑어본것

# none
---
  스키마
    NoSQL과 같은 schema free를 지원
    다중 스키마 지원은무슨의미?

  조인
    parent_typ, child_type이용해서 지원

  실시간의미: 데이터는 색인 자업이 완료됨과 동시에 바로 검색할수있음,
              별도의 재시작이나 상태갱신이 필요치않음
              엘라스틱서치에서 데이터 저장자체가 색인작업이기때문(결과는 원본, 색인된자료)
              
  리커버리
  설정된 클러스터의 상태를 유지하기위해 데이터 복사, 재배치하는 활동
  환경설정파일, 게이트웨이 관련 설정으로 트리거 설정.

  디스커버리
  원격 네트워크에있는 노드와의 바인딩설정

  슬로우로그
  log4j사용되어 기록됨. 대상은 query, fetch, indexing 활동 


  플러그인

    bin/plugin 실행파일로 관리
  
    대상 remote주소가 download.elasticsearch, maven, github 세가지임

    


    헤드: es의 시스템구조를 파악하는데 도움주는 플러그인

  데이터색인(데이터가 저장될때 색인되는것)
    텀주의
      형태소분석 설정에 따라 저장형태가 달라짐
      기본이 공백기준으로 term구분하고 소문자 저장인듯함




# 용량 주의  
  ./data
    용량주의, 실제 색인데이터가 저장되는 위치, 실행자와, 데이터저장소를 분리하는것 권함

  메모리

    불필요한 오버헤드 피하기위해 최소,최대 메모리 동일하게 설정하는것을 권장

    jvm메모리고정필요
      bootstrap.mlockal: true // jvm에서 엘라스틱 프로세스의 메모리 위치 고정, 다른 프로세스가
      사용x

  자바힙덤프파일위치(힙메모리 오류발생시 생성됨)
  : heapdump.hdrof 파일이 수백~수GB이므로 저장위치 고려할것(기본은 설치된 위치)
    

# 파일구조
---
  bin
    elasticsearch.in.sh: start 관련 환경 설정

# 환경
  config/elasticsearch.yml


  경로
  사용되는 경로지정가능


# 데이터구조
  여러개의 도큐먼트가 하나의 타입을이루고, 여러개의 타입이 하나의 인덱스를 이룸
  인덱스는 샤드와 레플리카(복사본)로 구성되는 큰 데이터단위

  관계형 디비와의 구조비교
    관계형DB, 엘라스틱서치

    데이터베이스, 인덱스
    테이블, 타입
    행(row), 도큐먼트 !! 이게맞고 책이 잘못표기된듯
    열(column), 필드
    스키마, 매핑

    index 같은용어이기에 책에서는 아래의 두가지 단어로 구분
      데이터 구조는 인덱스
      데이터 추출 과정과 저장 공간은 색인


  인덱스
  타입
  도큐먼트


# 시스템구조

  일반적설계
  : 마스터노드 명령수행창구, 데이터노드 http통신을 막아 데이터저장역할만전담,
    ?? 마스터노드가 http통신으로 REST API 명령수행용이라는건지, 별도 노드가또있다는건지.

  클러스터
  : 여러 노드로 이루어진 독립된 시스템(서로 다른 클러스터에서는 서로달느
  프로세스라고생각하면될듯)

  노드
  : 각 노드는 하나의 엘라스틱서치 프로세스로 실행됨
    노드는 마스터, 슬레이브관계
    설정된 클러스터명으로 노드간 바인딩이 자동으로됨

  샤드
  데이터검색을 위해 구분되는 최소의 단위 인스턴스
  인덱스(물리데이터)가 1:1로 샤드(메모리데이터)가 되는것인듯
  엘라스틱서치에 색인된 데이터는 여러개의 샤드로 분할돼 저장되는데, 기본적으로 인덱스당
  5개의 샤드와 5개의 복사본으로 분리된다.
  사용자는 인덱스단위로 데이터를 처리하고, 샤드는 엘라스틱서치가 직접노드로 분산시키는
  작업을함

  최초샤드(primary shard)
  처음 데이터가 색인되어 자ㅓ장되는공간
  최초샤드에 데이터 색인되면 다시 동일한 최초샤드수만큼 복사본생성함
  기본적으로 같은 데이터블록의 최초샤드와 복사본은 서로다른 노드에 저장됨(실행중 노드가 하나면
  복사본생성X)
  기본적으로 최초샤드와 


  네트워크의 같은 클러스터의 바인딩
  : 바인딩위해 엘라스틱서치 버전이 같아야지

    같은네트워크일경우(물리서버가 다름)
    : 젠 디스커버리(rk
      멀티캐스트(동적으로 같은네트워크에서 탐색및연결), 유니캐스트(정적연결)
      포트 주의. 

# REST API
  1.4.2 REST API 정리 굳

  HTTP 노드 범위(REST API를 위한)
  : 포트가 노드 증가시 순차적으로 증가하여 실행됨 9200, 9201
    http.enabled: false|true  각 노드별로 제어가능

# CRUD(REST API 사용한)
  업데이트
  _version이 증가함

  삭제는
    도큐먼트단위
    실제 삭제가 아닌 _source에 입력된 값이 빈값으로 업데이트되고 검색되지않게 상태가변경되는것
    타입, 인덱스
    메타데이터까지 삭제됨, 둘이 좀 차이있음

  벌크
  여러명령을 한꺼번에 실행할수있음

  하나의 명령은 메타정보(crud관련명령)와 요청데이터(cu작업시) 쌍으로 이루어짐
  api따라 몇몇 메타정보는 생략가능

  udp를 사용할수도있음

    성능
    엘라스틱서치가 설치된 시스템 하드웨어에 영향을받음
    통상적으로 1000~5000정도가 적절함. 10000이상은 오류발생확률높음.

# 검색

  멀티테넌시
  타입, 인덱스, 여럿의 인덱스를 묶어서 멀티 인덱스 범위로 질의할수있다.
  여러 인덱스를 묶어서 처리하는 엘라스틱서치의 특징을 멀티테넌시라고함

  query
  url방식과 requestbody query 방식이있음

    explain: 질의에대한 상세설명포함
    timeout: 기본값은 결과가나올때까지 무한정기다림
    size: result doc의 양(default 10), max_content_length와 연관
    search_type: 검색수행방법

    필드선택
    fileds류와 _source는 결과형태 다름
      _source: 위의 fields와 비슷, 와일드카드(*,?), exclude/include 사용가능
      partial_fields: _source와 유사
      fields: 일반적인 select처럼 필드명 선택

    highlight: 결과에서 검색된텍스트 부분을 em태그(기본)로 둘러쌈

  result
    took: 걸린시간(밀리초 1/1000초)단위
    value: 검색어에 해당하는 데이터의 정확도계산한값. explain사용시만나타남?
      
# 검색_페이셋(deprecate), 집합(aggregations)
  기본 검색결과에 연산처리한 결과를 반환

  페이셋
  order: sort대신 페이셋안에 작성해야하는것같음

    term: 검색결과를 텀별로 구분해서 표시
    range: 범위, 숫자 날짜 형식만가능
      from(<=)..values..to(<)

    histogram: interval(간격)으로 축약하는듯?
    filter: 필터링
    match: A and B 같은것
    통계: 통계값
    geo: 위치, 거리의 기본단위는 미터(m)
      distance_type: 정밀도

  어그리게이션
  sub-aggregation을 줄수있지만 레벨이 깊어질수록 메모리등의 자원소비가 늘어날수있으므로 주의

    글로벌어그리게이션
    : 질의(query로 term등을 준것)의 결과와는 별도로
      global 어그리게이션의 결과에 대해서만 하나의 버킷을 생성함,
      한번에 질의, 어그리게이션이라는 두개의 별도 결과를 얻을수있다.

    버킷
    : document를 버킷이라는 저장소 단위로 구분해서 담아 새로운 데이터집합형성
      filter: 
      missing: 필드나 필드값이 없는 것에대한 버킷을 생성,
      terms: 검색된 텀별로 버킷을생성함
      ranges
      histogram
      geo_distance - geo_location: 위치 거리, 원형영역
      gehhash_location - geohash_grid: 위치 해시 그리드, 사각형 영역

    메트릭

      min
      max
      sum
      avg
      value_count
      extended_stats: 제곱합, 변위, 표준편차

      
# QueryDSL
성능적으로 필터는 캐시사용, 스코어계산안함이라 쿼리보다 빠르다함
일반적으로 y/n같은 바이너리 조건검색에는 필터, 복잡한질의는 쿼리를 사용한다고함
일반적으로 검색범위를 좁히는 목적으로는  필터, 검색 상세조건입력하고 도출된 결과를 분석하기
위한 목적으로 쿼리를 사용함


  쿼리
    텀쿼리, 매치쿼리
    텀쿼리는 문자그대로사용되고 매치쿼리는 쿼리의 텀도 형태소분석을거친다
    매치쿼리에서 어떻게 형태소분석할것인지 설정할수있음

    불쿼리: 다른쿼리를 불조합으로 적용해서 최종검색결과나타냄
    문자열쿼리
    접두어쿼리: 텀쿼리처럼 형태소분석X
    범위쿼리
    퍼지(fuzzy)쿼리, fuzzyness로 숫자범위 찾는것도있다.




  필터
    텀즈
    범위
    and, or, not: ??앞서 처리된 필터를 다시한번 비교하는필터이기에 캐싱X
    불필터:
      and, not, or와 유사
      must, must_not, should

      어느곳에사용할까
      (내부적으로)비트셋을 사용하는 텀, 범위 필터 등의 논리연산은 불 필터를,
      위치(geo_*) 필터나 스크립트 필터와 같이 비트셋 사용않는 필터는 and,or,not 필터 사용


    위치필터: 도큐먼트의 위치정보로 검색범위를 설정할수있다
      geo_distance: 한지점기준 떨어진거리
      geo_distance_range: from, to로 범위지정가능
      geo_bounding_box: 사각형
      geo_polygon: 여러지점연결하여 다각형범위


# 매핑
매핑은 데이터의 저장 형태와 검색 엔진에서 해당 데이터에 어떻게 접근하고 처리하는지에 대한
명세.
rdb의 스키마?

  설정방법
    인덱스 생성하면서 매핑설정
    _mapping API 사용
  
  매핑주의
    한번 설정된 매핑에 필드를 추가할수있지만, 변경, 삭제는 불가함.
    만약 delete메서드로 매핑삭제 시도시 해당 인덱스/타입의 데이터가 모두삭제됨.

    날짜는 형식주의하지 않으면 string 타입이 매핑되어 검색못하게됨
  
  내장필드
  매핑api에서 underbar로 시작, 컬럼정의

    _id
    document의 id를 의미
    path로 기존의 필드의 값을 id값으로 설정할수있음. 중복은??
      
    _source
    데이터의 원본이 _source로 저장되며 enabled, includes, excludes로 제어가능

    _all
    properties와 필드의 설정(include_in_all)을 이용하여 검색대상에서 필드를 제외할수있다.

    _analzer
    도큐먼트가 색인될때 사용할 분석기를 설정함

    _timestamp
    데이터 입력 시간의 시간 타임스탬프 저장제어

  데이터타입
  인덱스 생성시 데이터매핑정보, properties 사용
  필드옵션으로 가중치, 해석기등이 제어됨

    문자열, 숫자 등...검색에 영향을주므로 필드옵션 주의해야함

    객체타입은 properties로 트리형태의 하위 데이터타입 설정가능

    중첩타입
      properties로 설정
      데이터가 객체의 배열로 입력되고 색인되면서 flatten된다
      질의시 nested타입, 상위필드명을 입력해야함.

    geo_point
      경도, 위도로 설정하거나 geohash값으로 입력

      경도의 표준범위, 표준화옵션이있다
      100과 460. 100에서 360도를 더하고, 표준화하면 100이라고함. 검색되었는데 360초과값이면
      이런 표준화를 의심해봐야.함
    
    geo_shape
    선, 원, 사각, 다각형등의 위치정보 저장간으한 타입

    다중필드
    다중필드 설정하면 해당필드의 값한개가 여러곳(설정한 필드들)에 옵션에 맞처서 저장됨
    예로, title을 title_raw의 다중필드 추가하여 형태소분석옵션, 안분석옵션으로 두 필드를
    만들수있따.

      토큰수: 해석기 사용시 분리된 토큰 수 저장하는 그런거인듯

    필드복사
    copy_to, store
    저장시, 여러 필드의 값을 지정한 하나의 값으로 모을수있음
    저장시, 한필드의 값을 여러필드에 복사할수있음

# 분석과정
색인과정 중 입력된 데이터에서 검색어를 추출하기 위한 프로세스
인덱스에 설정된 분석기의 종류에따라 토크나이저(0 or 1개) -> 토큰필터(0 or 1개이상) 순의 과정

  분석기: 분석과정에서 사용하는 프로그램(토크나이저 + 토큰필터)

    토크나이저(tokenizer): 데이터를 토큰으로분리
      whitespace: 공백기준
      etc..

    토큰필터(tokenfilter): 분리된 토큰에 필터적용하여 실제 검색에 쓰이는 검색어로 최종
    변환함, 여러개 있을시 순차적으로 실행됨.
      lowercase: 대문자 -> 소문자
      stop: 영문중 stopwords에 해당하는(the, and 같은 대명사, 전치사등)을 제외
      word_delimiter: 이미 토큰화된 단어를 더 세부적으로 분할, 병합하는 기능
      합성어: 지정된 합성어 등장시 분리해줌
      etc..


  기본제공분석기: standard, simple, whitespace, stop, keyword, pattern, 다국어, cjk,
  snowball..

  사용자 정의 분석기
    토크나이저, 토큰필터 조합해서 만들수도있고,  
    기본제공분석기를 설정만 오버라이딩해서 확장할수있는듯하다.
    settings:anlysis:analyzer~

  한글분석기 플러그인: github-elasticsearch-analysis-korean

# ELK 스택
: 엘라스틱서치 + 로그스태시 + 키바나
  입력을 위해 일반데이터 -> json 가공을 간편히(로그스태시), 검색데이터를 편리하게
  시각화할수있는도구(키바나)

